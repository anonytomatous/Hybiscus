{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "\n",
    "Lang = 'Java'\n",
    "#Lang = 'C'\n",
    "\n",
    "if Lang == 'Java':\n",
    "    projects = ['Lang', 'Chart', 'Time', 'Math', 'Closure']\n",
    "    N = 8\n",
    "else:\n",
    "    projects = ['grep', 'gzip', 'sed', 'flex']\n",
    "    N = 5\n",
    "\n",
    "meta = pd.DataFrame()\n",
    "result = pd.DataFrame()\n",
    "fl_result = pd.DataFrame()\n",
    "dc_result = pd.DataFrame()\n",
    "for project in projects:\n",
    "    for num_faults in range(1, N+1):\n",
    "        if Lang == 'Java':\n",
    "            dataset_id = f\"{project}-faults-{num_faults}\"\n",
    "        else:\n",
    "            dataset_id = f\"{project}-faults-{num_faults}\"\n",
    "        mp, rp, flp, dcp = (\n",
    "            f\"resources/result/{dataset_id}_meta.pkl\",\n",
    "            f\"resources/result/{dataset_id}_result.pkl\",\n",
    "            f\"resources/result/{dataset_id}_fl_result.pkl\",\n",
    "            f\"resources/result/{dataset_id}_dist_cost_result.pkl\")\n",
    "        if os.path.exists(mp) and os.path.exists(rp):\n",
    "            t_meta = pd.read_pickle(mp)\n",
    "            t_result = pd.read_pickle(rp)\n",
    "\n",
    "            print(dataset_id, t_meta.shape[0])\n",
    "            t_meta['project'] = project\n",
    "            meta = meta.append(t_meta, ignore_index=True)\n",
    "            result = result.append(t_result, ignore_index=True)\n",
    "            if os.path.exists(flp):\n",
    "                t_fl_result = pd.read_pickle(flp)\n",
    "                fl_result = fl_result.append(t_fl_result, ignore_index=True)\n",
    "            if os.path.exists(dcp):\n",
    "                t_dc_result = pd.read_pickle(dcp)\n",
    "                dc_result = dc_result.append(t_dc_result, ignore_index=True)\n",
    "        else:\n",
    "            print(dataset_id, \"No result file\")\n",
    "\n",
    "assert meta['data_id'].unique().shape[0] == meta.shape[0]\n",
    "\n",
    "print(\"# total data points:\", meta.shape[0])\n",
    "for k, df in meta.groupby(['num_faults']):\n",
    "    print(f\"{k} faults: {df.shape[0]}\", \n",
    "        [ f\"{project}: {df[df.project == project].shape[0]}\" for project in projects])\n",
    "\n",
    "print(np.all(result.normalized_mutual_info_score - result.v_measure_score < 0.0001))\n",
    "\n",
    "print(meta.groupby(['project', 'num_faults'])['data_id'].nunique().reset_index().to_markdown(index=False))\n",
    "\n",
    "affinity_name_map = {'nlink': 'hdist'}\n",
    "for affinity in affinity_name_map:\n",
    "    new_name = affinity_name_map[affinity]\n",
    "    result.loc[result.affinity == affinity, 'affinity'] = new_name\n",
    "    dc_result.loc[dc_result.affinity == affinity, 'affinity'] = new_name"
   ]
  },
  {
   "source": [
    "# Subject Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Averaged # failing test cases for each # faults"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total:\", pd.to_numeric(meta.set_index(['data_id', 'project', 'num_faults']).num_vertices).mean())\n",
    "\n",
    "pd.to_numeric(meta.set_index(['data_id', 'project', 'num_faults']).num_vertices).mean(level=['num_faults', 'project']).to_frame().round(2)"
   ]
  },
  {
   "source": [
    "## Averaged # lines (components) per projects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total:\", pd.to_numeric(meta.set_index(['data_id']).num_total_components).mean())\n",
    "meta['num_total_components'] = meta['num_total_components'].astype('int32')\n",
    "meta.groupby(['project'])['num_total_components'].mean().to_frame().round(1)"
   ]
  },
  {
   "source": [
    "## the # data points for each # failing test cases"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta[meta['num_vertices'] <= 2].shape[0]/meta.shape[0])\n",
    "\n",
    "meta.groupby(['num_vertices']).count()['data_id'].to_frame()"
   ]
  },
  {
   "source": [
    "# Some helper functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filtering(result_df, filters_list):\n",
    "    sdf = pd.DataFrame()\n",
    "    for filters in filters_list:\n",
    "        tmp_df = result_df\n",
    "        method = '-'.join(filters.values())\n",
    "        for column in filters:\n",
    "            tmp_df = tmp_df[tmp_df[column] == filters[column]]\n",
    "        tmp_df['method'] = method\n",
    "        sdf = sdf.append(tmp_df, ignore_index=True)\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_at_max_modularity(result_df, filters=None):\n",
    "    if filters is not None:\n",
    "        result_df = filtering(result_df, [filters])\n",
    "    return result_df.sort_values(['modularity', 'iteration'], ascending=[False, True])\\\n",
    "        .groupby(['data_id']).first().reset_index()\n",
    "\n",
    "def stop_with_distance_threshold(result_df, filters=None, threshold=0.5):\n",
    "    if filters is not None:\n",
    "        result_df = filtering(result_df, [filters])\n",
    "    tdf = result_df[result_df.min_dist >= threshold]\n",
    "    return tdf.sort_values(\"iteration\").groupby(['data_id']).first().reset_index()\n",
    "\n",
    "def stop_at_min_dist_knee(result_df, filters=None, threshold=None):\n",
    "    \"\"\"\n",
    "    increasing curve\n",
    "    \"\"\"\n",
    "    if filters is not None:\n",
    "        result_df = filtering(result_df, [filters])\n",
    "    tdf = pd.DataFrame()\n",
    "    for k, g in result_df.groupby(['data_id', 'method']):\n",
    "        y = g.min_dist.values\n",
    "        y[-1] = 1.\n",
    "        diff = np.diff(np.insert(y, 0, 0.), 1) # first derivative\n",
    "        if threshold is not None:\n",
    "            diff[y < threshold] = 0\n",
    "        knee = diff.argmax()\n",
    "        tdf = tdf.append(g[g.iteration==knee], ignore_index=True)\n",
    "    return tdf\n",
    "\n",
    "def stop_at_hncut_knee(result_df, filters=None):\n",
    "    \"\"\"\n",
    "    decreasing\n",
    "    \"\"\"\n",
    "    if filters is not None:\n",
    "        result_df = filtering(result_df, [filters])\n",
    "    result_df['num_clusters'] = result_df['labels_pred'].apply(lambda t: len(set(t)))\n",
    "    tdf = pd.DataFrame()\n",
    "    for k, g in result_df.groupby(['data_id', 'method']):\n",
    "        y = g.hNCut.values/g.num_clusters.values # hncut normalization\n",
    "        diff = np.insert(y[0:-1], 0, 1.) - y\n",
    "        tdf = tdf.append(g[g.iteration==diff.argmax()], ignore_index=True)\n",
    "    return tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_Z(labels_list, distances):\n",
    "    def sort_clustering_labels(labels):\n",
    "        map_to_new_index = {}\n",
    "        for c in labels:\n",
    "            if c in map_to_new_index:\n",
    "                continue\n",
    "            else:\n",
    "                map_to_new_index[c] = len(map_to_new_index)\n",
    "        return [map_to_new_index[c] for c in labels]\n",
    "\n",
    "    labels_list = [sort_clustering_labels(labels) for labels in labels_list]\n",
    "\n",
    "    num_clusters = None\n",
    "    for labels in labels_list:\n",
    "        if num_clusters is None:\n",
    "            num_clusters = len(set(labels))\n",
    "        else:\n",
    "            assert len(set(labels)) == num_clusters - 1\n",
    "            num_clusters -= 1\n",
    "    \n",
    "    cluster_assignment = np.array(range(0, len(labels_list[0])))\n",
    "    Z = []\n",
    "    prev_label = np.array(labels_list[0])\n",
    "    new_cluster_index = len(set(prev_label))\n",
    "    for i in range(1, len(labels_list)):\n",
    "        curr_label = np.array(labels_list[i])\n",
    "        unique_clusters = np.unique(curr_label)\n",
    "        merged_cluster = None\n",
    "        for c in unique_clusters:\n",
    "            child_labels = np.unique(prev_label[curr_label == c])\n",
    "            assert child_labels.shape[0] in [1, 2]\n",
    "            if child_labels.shape[0] == 2:\n",
    "                assert merged_cluster is None\n",
    "                merged_cluster = c\n",
    "        children = np.unique(cluster_assignment[curr_label == merged_cluster])\n",
    "        cluster_assignment[curr_label == merged_cluster] = new_cluster_index\n",
    "        Z.append([children[0], children[1], round(distances[i-1], 3), (cluster_assignment == new_cluster_index).sum()])\n",
    "        new_cluster_index += 1\n",
    "        prev_label = curr_label\n",
    "    return np.array(Z)\n",
    "\n",
    "def construct_linkage_matrix(result_df, data_id, affinity, linkage):\n",
    "    tdf = filtering(result_df, [{'clustering': 'Agglomerative', 'affinity': affinity, 'linkage': linkage, 'data_id': data_id}])\n",
    "    return construct_Z(tdf.labels_pred.tolist(), tdf.min_dist.tolist())\n",
    "\n",
    "if Lang == 'Java':\n",
    "    print(construct_linkage_matrix(result, 'Chart10b-Chart11b-Chart12b', 'hdist', 'average'))"
   ]
  },
  {
   "source": [
    "# Drawing Dendrogram"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendogram(meta_df, result_df, data_id, affinity, linkage,\n",
    "                   show_testname=True, color_threshold='auto'):\n",
    "    # https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/#Inconsistency-Method\n",
    "    # Annotating dendrogram\n",
    "    if not np.any(data_id == meta_df.data_id):\n",
    "        raise Exception(f\"No such data: {data_id}\")\n",
    "    colors = ['tab:green', 'tab:orange', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:skyblue']\n",
    "    fault_names = meta_df[meta_df.data_id == data_id].faults.item()\n",
    "    ground_truth = meta_df[meta_df.data_id == data_id].labels_true.item()\n",
    "    test_names = meta_df[meta_df.data_id == data_id].failing_tests.item()\n",
    "    Z = construct_linkage_matrix(result_df, data_id, affinity, linkage)\n",
    "    mdist = Z[:,2]\n",
    "\n",
    "    mdist_diff = np.diff([0] + mdist.tolist() + [1], 1) # first derivative\n",
    "    mdist_elbow = mdist_diff.argmax()\n",
    "    final_k = len(mdist_diff) - mdist_elbow\n",
    "\n",
    "    if color_threshold == 'auto':\n",
    "        if mdist_elbow == len(mdist):\n",
    "            color_threshold = (1 + mdist[mdist_elbow-1])/2\n",
    "        elif mdist_elbow == 0:\n",
    "            color_threshold = mdist[1]/2\n",
    "        else:\n",
    "            color_threshold = (mdist[mdist_elbow] + mdist[mdist_elbow-1])/2\n",
    "    print(f\"K = {final_k}\")\n",
    "    print(f\"GT K = {len(fault_names)}\")\n",
    "\n",
    "    plt.figure(\n",
    "        figsize=(10,min(0.5*len(mdist_diff), 10)))\n",
    "    plt.title(f'{data_id} ({affinity}-{linkage}): K={final_k}')\n",
    "    plt.ylabel('Failing Tests')\n",
    "    plt.xlabel('Distance')\n",
    "\n",
    "    labels = []\n",
    "    label_colors = {}\n",
    "    for i in range(len(test_names)):\n",
    "        root_cause = fault_names[ground_truth[i]]\n",
    "        if show_testname:\n",
    "            testname = test_names[i].split('.')[-1]\n",
    "            if len(testname) > 12:\n",
    "                testname = testname[:9]+'...'\n",
    "            label = f\"{testname} ({root_cause})\"\n",
    "        else:\n",
    "            label = f\"t{i+1}\\n({root_cause})\"\n",
    "        labels.append(label)\n",
    "        label_colors[label] = colors[ground_truth[i]]\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        #leaf_font_size=8.,  # font size for the x axis labels\n",
    "        orientation='left',\n",
    "        labels=labels,\n",
    "        color_threshold=color_threshold,\n",
    "        #link_color_func=lambda k: 'black'\n",
    "    )\n",
    "    plt.plot([color_threshold, color_threshold], [0, 10*(Z.shape[0] + 1)], c='red')\n",
    "    for text in plt.yticks()[1]:\n",
    "        #print(text.__dict__)\n",
    "        text._color = label_colors[text._text]\n",
    "    plt.xlim((1.1, 0))\n",
    "    plt.grid(True, axis='x')\n",
    "    #ticks = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    #plt.xticks(ticks=ticks, labels=[str(round(1-t, 1)) for t in ticks])\n",
    "    plt.savefig(f'figures/dend/{data_id}_{affinity}_{linkage}.pdf', bbox_inches=\"tight\")\n",
    "    plt.savefig(f'figures/dend/{data_id}_{affinity}_{linkage}.png', bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_objectives(meta_df, result_df, data_id, affinity, linkage,\n",
    "                    objectives=['modularity', 'hNCut', 'min_dist'],\n",
    "                    eval_metrics=['normalized_mutual_info_score']):\n",
    "    tdf = filtering(result_df, \n",
    "        [{'clustering': 'Agglomerative', 'affinity': affinity, 'linkage': linkage, 'data_id': data_id}])\n",
    "    tdf['num_clusters'] = tdf['labels_pred'].apply(lambda t: len(set(t)))\n",
    "    melted = pd.melt(tdf, id_vars=['num_clusters'],\n",
    "                     value_vars=objectives + eval_metrics,\n",
    "                     var_name='Metric', value_name='Value')\n",
    "    melted['Type'] = melted.Metric.apply(lambda m: 'Objective' if m in objectives else 'Score')\n",
    "    plt.title(f'{data_id} using {affinity} ({linkage})')\n",
    "    plt.xticks(range(1, melted.num_clusters.max() + 1))\n",
    "    sns.lineplot(data=melted, x='num_clusters', y='Value', hue='Metric', style='Type', marker='o')\n",
    "    plt.legend(bbox_to_anchor=(1.2, 0.35, 0.25, 0.25), loc='center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_id in meta[(meta.num_faults >= 3) & (meta.project == 'Closure')].data_id[:100]:\n",
    "    plot_dendogram(meta, result, data_id, 'hdist', 'average', show_testname=True)\n",
    "    #plot_dendogram(meta, result, data_id, 'RKT', 'average', show_testname=True)"
   ]
  },
  {
   "source": [
    "# Evaluate Stopping Criteria"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_stopping_criteria(meta_df, result_df, affinity, linkage,\n",
    "                               evaluation_metric='normalized_mutual_info_score',\n",
    "                               project=None, num_faults=None):\n",
    "    tdf = filtering(result_df, [\n",
    "        {'clustering': 'Agglomerative', 'affinity': affinity, 'linkage': linkage}])\n",
    "\n",
    "    tdf = tdf.join(meta_df.set_index('data_id'), on='data_id')\n",
    "    if project is not None:\n",
    "        tdf = tdf[tdf.project==project]\n",
    "    if num_faults is not None:\n",
    "        tdf = tdf[tdf.num_faults==num_faults]\n",
    "\n",
    "    N = tdf.data_id.unique().shape[0]\n",
    "\n",
    "    tdf['num_clusters'] = tdf['labels_pred'].apply(lambda t: len(set(t)))\n",
    "    tdf['num_clusters/num_faults'] = tdf['num_clusters']/tdf['num_faults']\n",
    "    tdf['num_clusters==num_faults'] = tdf['num_clusters']==tdf['num_faults']\n",
    "\n",
    "    if evaluation_metric == 'perfect_ratio':\n",
    "        tdf['perfect_ratio'] = tdf['normalized_mutual_info_score'] == 1\n",
    "\n",
    "    eval_result = pd.DataFrame([], columns=['stopping_criterion', f\"mean\", f\"std\"])\n",
    "\n",
    "    # optimal\n",
    "    optimal_predictions = tdf.sort_values(evaluation_metric, ascending=False).groupby(['data_id']).first()\n",
    "    assert optimal_predictions.shape[0] == N\n",
    "    eval_result = eval_result.append({\n",
    "        'stopping_criterion': 'optimal',\n",
    "        f'mean': optimal_predictions[evaluation_metric].mean(),\n",
    "        f'std': optimal_predictions[evaluation_metric].std()\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # optimal\n",
    "    optimal_predictions = tdf[tdf.num_vertices > 2]\\\n",
    "        .sort_values(evaluation_metric, ascending=False).groupby(['data_id']).first()\n",
    "    assert optimal_predictions.shape[0] == (meta['num_vertices'] > 2).sum()\n",
    "    eval_result = eval_result.append({\n",
    "        'stopping_criterion': 'optimal (>2 failings)',\n",
    "        f'mean': optimal_predictions[evaluation_metric].mean(),\n",
    "        f'std': optimal_predictions[evaluation_metric].std()\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # distance threshold\n",
    "    tick = 0.05\n",
    "    threshold = 0\n",
    "    while threshold <= 1:\n",
    "        predictions = stop_with_distance_threshold(tdf, threshold=threshold)\n",
    "        assert predictions.shape[0] == N\n",
    "        eval_result = eval_result.append({\n",
    "            'stopping_criterion': 'distance_threshold_{:.2f}'.format(threshold),\n",
    "            'threshold': threshold,\n",
    "            f'mean': predictions[evaluation_metric].mean(),\n",
    "            f'std': predictions[evaluation_metric].std()\n",
    "        }, ignore_index=True)\n",
    "        threshold += tick\n",
    "\n",
    "    # maximum modularity\n",
    "    predictions = stop_at_max_modularity(tdf)\n",
    "    assert predictions.shape[0] == N\n",
    "    eval_result = eval_result.append({\n",
    "        'stopping_criterion': 'max_modularity',\n",
    "        f'mean': predictions[evaluation_metric].mean(),\n",
    "        f'std': predictions[evaluation_metric].std()\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # # hncut knee\n",
    "    # predictions = stop_at_hncut_knee(tdf)\n",
    "    # assert predictions.shape[0] == N\n",
    "    # eval_result = eval_result.append({\n",
    "    #     'stopping_criterion': 'hncut_knee',\n",
    "    #     f'mean': predictions[evaluation_metric].mean(),\n",
    "    #     f'std': predictions[evaluation_metric].std()\n",
    "    # }, ignore_index=True)\n",
    "\n",
    "    # min_dist knee\n",
    "    predictions = stop_at_min_dist_knee(tdf)\n",
    "    assert predictions.shape[0] == N\n",
    "    eval_result = eval_result.append({\n",
    "        'stopping_criterion': 'min_dist_knee',\n",
    "        f'mean': predictions[evaluation_metric].mean(),\n",
    "        f'std': predictions[evaluation_metric].std()\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    # distance threshold + min dist knee\n",
    "    # tick = 0.05\n",
    "    # threshold = 0\n",
    "    # while threshold <= 1:\n",
    "    #     predictions = stop_at_min_dist_knee(tdf, threshold=threshold)\n",
    "    #     assert predictions.shape[0] == N\n",
    "    #     eval_result = eval_result.append({\n",
    "    #         'stopping_criterion': 'min_dist_knee_after_{:.2f}'.format(threshold),\n",
    "    #         'threshold': threshold,\n",
    "    #         f'mean': predictions[evaluation_metric].mean(),\n",
    "    #         f'std': predictions[evaluation_metric].std()\n",
    "    #     }, ignore_index=True)\n",
    "    #     threshold += tick\n",
    "\n",
    "    if evaluation_metric == 'perfect_ratio':\n",
    "        return eval_result[['stopping_criterion', 'mean']]\n",
    "\n",
    "    return eval_result\n",
    "\n",
    "stop_eval_result = pd.DataFrame()\n",
    "affinities = ['hdist', 'jaccard', 'dice', 'cosine', 'hamming', 'euclidean', 'RKT']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "evaluation_metrics = ['normalized_mutual_info_score', 'perfect_ratio', 'homogeneity_score', 'completeness_score', 'num_clusters/num_faults', 'num_clusters==num_faults']\n",
    "for affinity, linkage, evaluation_metric in tqdm(list(product(affinities, linkages, evaluation_metrics))):\n",
    "    eval_result_df = evaluate_stopping_criteria(meta, result, affinity, linkage, evaluation_metric=evaluation_metric)#.sort_values('mean_normalized_mutual_info_score', ascending=False)\n",
    "    eval_result_df['affinity'] = affinity\n",
    "    eval_result_df['linkage'] = linkage\n",
    "    eval_result_df['evaluation_metric'] = evaluation_metric\n",
    "    stop_eval_result = stop_eval_result.append(eval_result_df, ignore_index=True)\n",
    "\n",
    "stop_eval_result"
   ]
  },
  {
   "source": [
    "# RQ1: The upper bound of clustering performance\n",
    "The above cell (**evaluate_stopping_criteria**) must have been executed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Barchart of Maximum NMI and perfect labelling ratio \n",
    "\n",
    "if you want to see the chart\n",
    "- for all subjects, set `stopping_criterion = 'optimal'` (in the paper)\n",
    "- for the subjects with > 2 failing test cases, set `stopping_criterion = 'optimal (>2 failings)'`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_affinities = ['jaccard', 'dice', 'cosine', 'euclidean', 'hamming', 'RKT', 'hdist']\n",
    "stopping_criterion = 'optimal' \n",
    "# stopping_criterion = 'optimal (>2 failings)'\n",
    "size = 'large'\n",
    "if size != 'large':\n",
    "    plt.figure(figsize=(8, 3))\n",
    "else:\n",
    "    # paper size\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "evaluation_metrics = ['normalized_mutual_info_score', 'perfect_ratio']\n",
    "print(stop_eval_result[\n",
    "    (stop_eval_result.stopping_criterion == stopping_criterion)\n",
    "    & (stop_eval_result.evaluation_metric.isin(evaluation_metrics))\n",
    "    & (stop_eval_result.affinity.isin(target_affinities))]\\\n",
    "    .pivot(index=['affinity', 'linkage'], columns='evaluation_metric', values='mean')\\\n",
    "    .reset_index()\\\n",
    "    #.sort_values(by='linkage', key=lambda col: [['single', 'average', 'complete'].index(v) for v in col])\n",
    "    .sort_values(by=['affinity'], key=lambda col: [target_affinities.index(v) for v in col])\\\n",
    "    .round(3).to_markdown(index=False))\n",
    "\n",
    "for i, evaluation_metric in enumerate(['normalized_mutual_info_score', 'perfect_ratio']):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    partial_stop_eval_result = stop_eval_result[\n",
    "        (stop_eval_result.stopping_criterion == stopping_criterion)\n",
    "        & (stop_eval_result.evaluation_metric == evaluation_metric)\n",
    "        & (stop_eval_result.affinity.isin(target_affinities))]\n",
    "    plt.grid(True, axis='y', zorder=-3)\n",
    "\n",
    "    sns.barplot(data=partial_stop_eval_result,\n",
    "        x='affinity', y='mean', hue='linkage', order=target_affinities, zorder=5)\n",
    "    plt.ylim((max(partial_stop_eval_result['mean'].min() - 0.01, 0), min(partial_stop_eval_result['mean'].max() + 0.025, 1)))\n",
    "    if 'optimal' in stopping_criterion and evaluation_metric == 'normalized_mutual_info_score':\n",
    "        plt.title(f\"Maximum NMI\", size=11)\n",
    "        plt.ylabel(\"Average maximum NMI\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    elif 'optimal' in stopping_criterion and evaluation_metric == 'perfect_ratio':\n",
    "        plt.title(f\"Ratio of Perfectly Clustered Subjects\", size=11)\n",
    "        plt.ylabel(\"Ratio\")\n",
    "        plt.legend(loc=\"upper left\")\n",
    "    else:\n",
    "        plt.title(f\"{evaluation_metric.replace('_', ' ').capitalize()}\\n{stopping_criterion} ({Lang} subjects)\")\n",
    "    #plt.xlabel(\"Distance Metric\")\n",
    "    plt.xlabel(None)\n",
    "    plt.xticks(rotation=50, size=11)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "if size == 'large':\n",
    "    plt.savefig(f\"figures/eval/{Lang}_{stopping_criterion}_large.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/eval/{Lang}_{stopping_criterion}_large.png\", bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f\"figures/eval/{Lang}_{stopping_criterion}.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/eval/{Lang}_{stopping_criterion}.png\", bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## RQ1 supplementary result: maximum NMI histogram"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def max_NMI_frame(meta_df, result_df, filters_list):\n",
    "    multi_faults = meta_df[(meta_df['num_vertices'] > 2)]['data_id']\n",
    "\n",
    "    sdf = filtering(result_df, filters_list)\n",
    "    mdf = sdf[sdf.data_id.isin(multi_faults)]\n",
    "\n",
    "    max_nmi_score = mdf.groupby(['data_id', 'method']).max()['normalized_mutual_info_score'].to_frame()\n",
    "    max_nmi_score[\"maximum NMI\"] = max_nmi_score[\"normalized_mutual_info_score\"]\n",
    "    return max_nmi_score.reset_index()\n",
    "\n",
    "def max_NMI_analysis(meta_df, result_df, filters_list):\n",
    "    max_nmi_score = max_NMI_frame(meta_df, result_df, filters_list)\n",
    "    \n",
    "    # T-Test\n",
    "    groups = max_nmi_score.groupby(\"method\")\n",
    "    for i, g1 in enumerate(groups):\n",
    "        method1, df1 = g1\n",
    "        for j, g2 in enumerate(groups):\n",
    "            method2, df2 = g2\n",
    "            if not i < j:\n",
    "                continue\n",
    "            \"\"\"\n",
    "            Calculate the t-test on TWO RELATED samples of scores, a and b.\n",
    "            \"\"\"\n",
    "            x, y = df1[\"maximum NMI\"].values, df2[\"maximum NMI\"].values\n",
    "            t_stat, p_value = stats.ttest_rel(x,y)\n",
    "            print(p_value)\n",
    "            if p_value < 0.05:\n",
    "                #print(f\"Reject the null hypothesis of identical average scores: {method1}, {method2}\")\n",
    "                if x.mean() < y.mean():\n",
    "                    print(f\"{method1} < {method2} (pvalue={round(p_value, 3)})\")\n",
    "                else:\n",
    "                    print(f\"{method2} < {method1} (pvalue={round(p_value, 3)})\")\n",
    "            else:\n",
    "                print(f\"{method1} ~ {method2} (accept null hypothesis)\")\n",
    "    # Histogram\n",
    "    plt.figure()\n",
    "    plt.title(\"Histogram of Maximum NMI scores\")\n",
    "\n",
    "    sns.histplot(data=max_nmi_score,\n",
    "                 x=\"maximum NMI\", binwidth=0.1,\n",
    "                 hue=\"method\", multiple=\"dodge\", kde=True,\n",
    "                 bins=np.arange(0, 1.1, 0.1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if Lang == 'Java':\n",
    "    max_NMI_analysis(meta, result, filters_list=[\n",
    "        {'clustering': 'Agglomerative','affinity': 'hdist', 'linkage': 'average'},\n",
    "        {'clustering': 'Agglomerative','affinity': 'RKT', 'linkage': 'single'}\n",
    "    ])\n",
    "elif Lang == 'C':\n",
    "    max_NMI_analysis(meta, result, filters_list=[\n",
    "        {'clustering': 'Agglomerative','affinity': 'hdist', 'linkage': 'single'},\n",
    "        {'clustering': 'Agglomerative','affinity': 'euclidean', 'linkage': 'average'}\n",
    "    ])"
   ]
  },
  {
   "source": [
    "# RQ2: Evaluating the result with a stopping criterion for agglomerative clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Evaluation of baseline techniques: MSeer and TCN (in the paper)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Lang == 'Java':\n",
    "    baselines = ['MSeer', 'TCN']\n",
    "else:\n",
    "    baselines = ['MSeer']\n",
    "\n",
    "result[\"perfect_ratio\"] = result[\"normalized_mutual_info_score\"].apply(lambda s: s == 1.0)\n",
    "\n",
    "result['num_clusters'] = result['labels_pred'].apply(lambda t: len(set(t)))\n",
    "result['num_clusters/num_faults'] = result['num_clusters']/result.join(meta.set_index('data_id'), on='data_id')['num_faults'].astype('float32')\n",
    "result['num_clusters==num_faults'] = (result['num_clusters']==result.join(meta.set_index('data_id'), on='data_id')['num_faults'])\n",
    "\n",
    "print(result[result.clustering.isin(baselines)].groupby(['clustering']).mean()\\\n",
    "    [['num_clusters/num_faults', 'num_clusters==num_faults', 'homogeneity_score', 'completeness_score', 'normalized_mutual_info_score', 'perfect_ratio']].round(3).reset_index().to_markdown(index=False))\n",
    "result[result.clustering.isin(baselines)].groupby(['clustering']).mean()\\\n",
    "    [['num_clusters/num_faults', 'num_clusters==num_faults', 'homogeneity_score', 'completeness_score', 'normalized_mutual_info_score', 'perfect_ratio']].round(3)"
   ]
  },
  {
   "source": [
    "## Using Algorithm 2: Elbow point of mdist curve (in the paper)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = stop_eval_result[\n",
    "    (stop_eval_result.stopping_criterion == 'min_dist_knee')\n",
    "    & (stop_eval_result.affinity != 'euclidean') # since it's unnormalised\n",
    "]\n",
    "evaluation_metrics = ['num_clusters/num_faults', 'num_clusters==num_faults', 'homogeneity_score', 'completeness_score', 'normalized_mutual_info_score', 'perfect_ratio']\n",
    "data = {}\n",
    "for m in evaluation_metrics:\n",
    "    mdf = tdf[stop_eval_result.evaluation_metric == m][['affinity', 'linkage', 'mean']].round(3)\n",
    "    data[m] = mdf.set_index(['affinity', 'linkage'])['mean'].rename(m)\n",
    "print(pd.concat(data, axis=1).reset_index().to_markdown(index=False))"
   ]
  },
  {
   "source": [
    "## Using max-modularity stopping criterion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = stop_eval_result[\n",
    "    (stop_eval_result.stopping_criterion == 'max_modularity')\n",
    "]\n",
    "evaluation_metrics = ['num_clusters/num_faults', 'num_clusters==num_faults', 'homogeneity_score', 'completeness_score', 'normalized_mutual_info_score', 'perfect_ratio']\n",
    "data = {}\n",
    "for m in evaluation_metrics:\n",
    "    mdf = tdf[stop_eval_result.evaluation_metric == m][['affinity', 'linkage', 'mean']].round(3)\n",
    "    data[m] = mdf.set_index(['affinity', 'linkage'])['mean'].rename(m)\n",
    "print(pd.concat(data, axis=1).reset_index().to_markdown(index=False))"
   ]
  },
  {
   "source": [
    "## Averaged NMI scores for each distance threshold"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metric = 'normalized_mutual_info_score' # 'perfect_ratio'\n",
    "plt.figure(figsize=(12, 10))\n",
    "stop_eval_result['is_dist_threshold'] = stop_eval_result['stopping_criterion']\\\n",
    "    .apply(lambda s: s.startswith(\"distance_threshold_\"))\n",
    "sns.lineplot(data=stop_eval_result[\n",
    "        stop_eval_result.is_dist_threshold\n",
    "        & (stop_eval_result.evaluation_metric == evaluation_metric)\n",
    "        & (stop_eval_result.affinity != 'euclidean') # since it's unnormalised\n",
    "    ], x='threshold', y='mean', hue='affinity', style='linkage')\n",
    "plt.title(\"NMI for each distance threshod\")\n",
    "plt.grid(True, axis='y')\n",
    "if Lang == 'Java':\n",
    "    plt.ylim((0.35, 0.9))\n",
    "\n",
    "else:\n",
    "    plt.ylim((0.20, 0.8))\n",
    "plt.ylabel(\"Averaged NMI\")\n",
    "plt.savefig(f\"figures/eval/NMI_distance_threshold_{Lang}.pdf\", bbox_inches='tight')\n",
    "plt.savefig(f\"figures/eval/NMI_distance_threshold_{Lang}.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "stop_eval_result[\n",
    "        stop_eval_result.is_dist_threshold\n",
    "        & (stop_eval_result.evaluation_metric == evaluation_metric)\n",
    "        & (stop_eval_result.affinity != 'euclidean') # since it's unnormalised\n",
    "].sort_values(['mean'], ascending=False)"
   ]
  },
  {
   "source": [
    "## Drawing Comparison Graphs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_to_method = {\n",
    "   'Agglomerative-max-modularity': stop_at_max_modularity,\n",
    "   'Agglomerative-elbow-method': stop_at_min_dist_knee\n",
    "}\n",
    "result = result[~result.clustering.isin(stopping_to_method.keys())]\n",
    "\n",
    "for key, df in result[result['clustering'] == 'Agglomerative'].groupby(['affinity', 'linkage']):\n",
    "   affinity, linkage = key\n",
    "   print(f\"Processing: {key}\")\n",
    "   for criterion in stopping_to_method:\n",
    "      if affinity == 'euclidean' and criterion != 'Agglomerative-max-modularity':\n",
    "         continue\n",
    "      df = filtering(result, [{'clustering': criterion, 'affinity': affinity, 'linkage': linkage}])\n",
    "      df = stopping_to_method[criterion](result,\n",
    "         {'clustering': 'Agglomerative', 'affinity': affinity, 'linkage': linkage})\n",
    "      df['clustering'] = criterion\n",
    "      df['min_dist'] = float(\"inf\")\n",
    "      result = result.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_stopping_criteria(meta_df, result_df, affinity, linkage,\n",
    "                   eval_metric='normalized_mutual_info_score',\n",
    "                   distance_thresholds=np.arange(0, 1.01, 0.05),\n",
    "                   savepath=None,\n",
    "                   show_errorband=True):\n",
    "    if affinity != 'euclidean':\n",
    "        filters_list=[\n",
    "            {'clustering': 'Agglomerative-elbow-method', 'affinity': affinity, 'linkage': linkage},\n",
    "            {'clustering': 'Agglomerative', 'affinity': affinity, 'linkage': linkage},\n",
    "            {'clustering': 'Agglomerative-max-modularity', 'affinity': affinity, 'linkage': linkage},\n",
    "        ]\n",
    "    else:\n",
    "        filters_list=[\n",
    "            {'clustering': 'Agglomerative-max-modularity', 'affinity': affinity, 'linkage': linkage},\n",
    "        ]\n",
    "    merged_frame = pd.DataFrame()\n",
    "\n",
    "    hue_order = []\n",
    "    for filters in filters_list:\n",
    "        for threshold in distance_thresholds:\n",
    "            pred = stop_with_distance_threshold(result_df, filters, threshold)\n",
    "            pred['threshold'] = threshold\n",
    "            merged_frame = merged_frame.append(pred)\n",
    "        hue_order.append(filters['clustering'])\n",
    "\n",
    "    merged_frame['perfect'] = merged_frame['normalized_mutual_info_score'] == 1\n",
    "    if not show_errorband or eval_metric == 'perfect':\n",
    "        merged_frame = merged_frame.groupby(['threshold', 'clustering']).mean()[eval_metric]\n",
    "        merged_frame = merged_frame.reset_index()\n",
    "\n",
    "    plt.title(f\"{affinity}-{linkage}\")\n",
    "    sns.lineplot(data=merged_frame, x='threshold', y=eval_metric,\n",
    "        hue='clustering', hue_order=hue_order, ci=68, legend=(linkage == 'complete'))\n",
    "    if linkage != 'single':\n",
    "        plt.ylabel(None)\n",
    "\n",
    "affinities = ['jaccard', 'dice', 'cosine', 'euclidean', 'hamming', 'RKT', 'hdist']\n",
    "linkages = ['single', 'average', 'complete']\n",
    "for affinity in affinities:\n",
    "    plt.figure(figsize=(11, 3))\n",
    "    for i, linkage in enumerate(linkages):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        compare_stopping_criteria(\n",
    "            meta, result, eval_metric='normalized_mutual_info_score',\n",
    "            affinity=affinity, linkage=linkage, show_errorband=False)\n",
    "    plt.legend(bbox_to_anchor=(1.5, 0.35, 0.25, 0.25), loc='center')\n",
    "    plt.savefig(f\"figures/eval/stop_{affinity}_{Lang}.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "# \\[RQ3\\] Analysis of Distance Calculation Cost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Run this cell only once, and draw the **distance calculation cost** graph (right below this cell) for both `Java` and `C`. Then results of both languages will be save in `merged_cost_df`. With the dataframe, you can draw the **RKT complexity analysis** graph."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cost_df = pd.DataFrame()"
   ]
  },
  {
   "source": [
    "## Distance calculation cost (in the paper)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dist = ['jaccard', 'dice', 'cosine', 'hamming', 'RKT', 'hdist']\n",
    "covered_reduction_needed_dist = set(target_dist) - set({'RKT', 'hdist'})\n",
    "\n",
    "joined_cost_df = dc_result[dc_result.affinity.isin(target_dist)].set_index(['data_id', 'affinity']).join(meta.set_index(['data_id']), on='data_id').reset_index()\n",
    "\n",
    "update_index = joined_cost_df.affinity == 'hdist'\n",
    "joined_cost_df.loc[update_index, 'cost'] = joined_cost_df[update_index].cost + joined_cost_df[update_index].hypergraph_modeling_cost\n",
    "\n",
    "update_index = joined_cost_df.affinity.isin(covered_reduction_needed_dist)\n",
    "joined_cost_df.loc[update_index, 'cost'] = joined_cost_df[update_index].cost + joined_cost_df[update_index].coverage_reduction_cost\n",
    "\n",
    "# joined_cost_df['log10(sec)'] = np.log10(joined_cost_df['cost'])\n",
    "\n",
    "joined_cost_df['Language'] = Lang\n",
    "if merged_cost_df.shape[0] == 0 or np.all(merged_cost_df['Language'] != Lang):\n",
    "    merged_cost_df = merged_cost_df.append(joined_cost_df, ignore_index=True)\n",
    "\n",
    "#plt.figure(figsize=(6, 2.5))\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(f\"Log-scaled distance calculation cost (s)\")\n",
    "\n",
    "major_logS = range(-4, 6, 2)\n",
    "minor_logS = range(-4, 5, 1)\n",
    "\n",
    "major_ticks = [10**n for n in major_logS]\n",
    "minor_ticks = [10**n for n in minor_logS]\n",
    "\n",
    "for i in range(len(major_ticks) - 1):\n",
    "    bins = 11\n",
    "    gap = major_ticks[i+1] - major_ticks[i]\n",
    "    width = gap/bins\n",
    "    ticks = [round(major_ticks[i]+width*j, 5) for j in range(0, bins)]\n",
    "    for tick in ticks:\n",
    "        plt.plot([tick, tick], [-0.5, 6.5], c='#ddd', zorder=-1, linestyle='-')\n",
    "\n",
    "for tick in [n for n in major_ticks]:\n",
    "    plt.plot([tick, tick], [-0.5, 6.5], c='#aaa', zorder=-1)\n",
    "\n",
    "# for n in logS:\n",
    "#     \"\"\"\n",
    "#     guide lines\n",
    "#     \"\"\"\n",
    "#     plt.plot([n, n], [-0.5, 6.5], c='lightgrey', zorder=-1)\n",
    "\n",
    "# 1. stripplot\n",
    "# sns.stripplot(data=merged_cost_df, y='affinity', x='log10(sec)', order=target_dist,\n",
    "#               hue='Language', dodge=True, alpha=.25, zorder=3)\n",
    "\n",
    "# 2. boxplot\n",
    "plt.xscale('log')\n",
    "#plt.grid(True, axis='x', which='both')\n",
    "ax = sns.boxplot(data=merged_cost_df, y='affinity', x='cost', order=target_dist, hue='Language', zorder=100)\n",
    "plt.xticks([n for n in minor_ticks], [\"1e{}s\".format(n) if abs(n) > 2 else (str(10**n) + \"s\") for n in minor_logS])\n",
    "\n",
    "plt.ylabel('Distance Metric')\n",
    "\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.savefig(f\"figures/eval/distance_cost_boxplot.pdf\", bbox_inches=\"tight\")\n",
    "plt.savefig(f\"figures/eval/distance_cost_boxplot.png\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(merged_cost_df[merged_cost_df.affinity == 'hdist'].cost < 1).all()"
   ]
  },
  {
   "source": [
    "## RKT complexity analysis (in the paper)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RKT_cost = merged_cost_df[merged_cost_df.affinity=='RKT']\n",
    "RKT_cost['squared M'] = np.power(RKT_cost['num_total_components'], 2)\n",
    "RKT_cost['squared |T_F|'] = np.power(RKT_cost['num_vertices'], 2)\n",
    "RKT_cost['cost_estimator'] = RKT_cost['squared M'] * RKT_cost['squared |T_F|']\n",
    "RKT_cost = RKT_cost.sort_values(['project'])\n",
    "\n",
    "plt.figure(figsize=(4,2.5))\n",
    "#plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.scatterplot(data=RKT_cost, x='cost_estimator', y='cost', hue='project', style='project', s=60)\n",
    "plt.title(f'Time complexity analysis of RKT')\n",
    "plt.xlabel(r'$M^2 \\times |T_F|^2$')\n",
    "plt.ylabel('RKT calculation time (s)')\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x=RKT_cost.cost_estimator.astype('float32'),\n",
    "                                                               y=RKT_cost.cost.astype('float32'))\n",
    "print(\"r_squared\", r_value ** 2)\n",
    "print(r_value, p_value, std_err)\n",
    "plt.plot(RKT_cost.cost_estimator.astype('float32'),\n",
    "         slope * RKT_cost.cost_estimator.astype('float32') + intercept,\n",
    "         zorder=-1, color='lightgrey', linestyle='dashed')\n",
    "plt.text(3.2 * 1e12, 6000, \"y = {:.2g}x {:.2g}\\n(R-squared: {:.4f})\".format(slope, intercept, r_value**2), color='#111')\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.38, 0.25, 0.25), loc='center')\n",
    "#plt.legend(bbox_to_anchor=(1.10, 0.35, 0.25, 0.25), loc='center')\n",
    "\n",
    "plt.grid(False)\n",
    "\n",
    "plt.savefig(f\"figures/eval/RKT_time_complexity.pdf\", bbox_inches='tight')\n",
    "plt.savefig(f\"figures/eval/RKT_time_complexity.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "hdist_cost = merged_cost_df[merged_cost_df.affinity=='hdist']\n",
    "hdist_cost['squared |T_F|'] = np.power(hdist_cost['num_vertices'], 2)\n",
    "X = hdist_cost[['num_hyperedges', 'num_total_components', 'squared |T_F|']]\n",
    "y = hdist_cost['cost']\n",
    "\n",
    "size = 'small'\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "a, b, c = linreg.coef_\n",
    "\n",
    "hdist_cost['cost_estimator'] = a * hdist_cost['num_hyperedges'] + b * hdist_cost['num_total_components'] + c * hdist_cost['squared |T_F|']\n",
    "\n",
    "hdist_cost = hdist_cost.sort_values(['project'])\n",
    "if size == 'large':\n",
    "    plt.figure(figsize=(10, 5))\n",
    "else:\n",
    "    plt.figure(figsize=(7,2))\n",
    "#plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.scatterplot(data=hdist_cost, x='cost_estimator', y='cost', hue='project', style='project', s=60)\n",
    "\n",
    "plt.title(f'Cost analysis of hdist')\n",
    "\n",
    "plt.ylabel('hdist calculation time (s)')\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x=hdist_cost.cost_estimator.astype('float32'),\n",
    "                                                               y=hdist_cost.cost.astype('float32'))\n",
    "print(\"r_squared\", r_value ** 2)\n",
    "print(r_value, p_value, std_err)\n",
    "plt.plot(hdist_cost.cost_estimator.astype('float32'),\n",
    "         slope * hdist_cost.cost_estimator.astype('float32') + intercept,\n",
    "         zorder=-1, color='lightgrey', linestyle='dashed')\n",
    "        \n",
    "\n",
    "if size == 'large':\n",
    "    plt.legend(bbox_to_anchor=(0.95, 0.35, 0.25, 0.25), loc='center')\n",
    "    plt.xlabel(r\"$({:.3g})M + ({:.3g})E + ({:.3g})|T_F|^2$\".format(b, c, a))\n",
    "    plt.ylabel('hdist calculation time (s)')\n",
    "    plt.text(0.5, 0.2, \"y = {:.3g}x {:.2g}\\n(R-squared: {:.4f})\".format(slope, intercept, r_value**2), color='#111')\n",
    "else:\n",
    "    plt.legend(bbox_to_anchor=(1.00, 0.35, 0.25, 0.25), loc='center')\n",
    "    plt.xlabel(r\"$x = 10^{}({:.2f}M' + {:.2f}M + {:.2f}|T_F|^2)$\".format(\"{-5}\", a*10**5, b*10**5, c*10**5))\n",
    "    plt.ylabel('time (s)')\n",
    "    plt.text(0.4, 0.0, \"time (s) = x {:.2g}\\n(R-squared: {:.4f})\".format(intercept, r_value**2), color='#111')\n",
    "\n",
    "# plt.grid(False)\n",
    "if size == 'large':\n",
    "    plt.savefig(f\"figures/eval/hdist_time_complexity_large.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/eval/hdist_time_complexity_large.png\", bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f\"figures/eval/hdist_time_complexity.pdf\", bbox_inches='tight')\n",
    "    plt.savefig(f\"figures/eval/hdist_time_complexity.png\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_cost_df[joined_cost_df.data_id == 'Closure49b-Closure50b']"
   ]
  },
  {
   "source": [
    "# \\[RQ4\\] Evaluate FL results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_figure(fl_eval_df, savepath=None):\n",
    "    melted = pd.DataFrame(columns=['clustering', 'metric', 'n', 'value'])\n",
    "    for i, row in fl_eval_df.iterrows():\n",
    "        clustering = row['clustering']\n",
    "        melted = melted.append({'clustering': clustering, 'metric': 'R.R.', 'value': row['R.R.']}, ignore_index=True)\n",
    "        for n in N:\n",
    "            melted = melted.append({'clustering': clustering, 'metric': 'found_faults', 'n': n, 'value': row[f'found_faults (n={n})']}, ignore_index=True)\n",
    "            melted = melted.append({'clustering': clustering, 'metric': 't_wef', 'n': n, 'value': row[f't_wef (n={n})']}, ignore_index=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    minor_ticks = [x+0.5 for x in range(melted.clustering.nunique())]\n",
    "\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title(\"Found Faults (higher is better)\")\n",
    "    ax = sns.barplot(data=melted[melted.metric == 'found_faults'], x='clustering', y='value', hue='n')\n",
    "    ax.set_xticks([], minor=False)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', axis='x')\n",
    "    plt.xlabel(None)\n",
    "    plt.legend(bbox_to_anchor=(0.93, 0.35, 0.25, 0.25), loc='center', title='n')\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.title(\"t-wef (lower is better)\")\n",
    "    ax = sns.barplot(data=melted[melted.metric == 't_wef'], x='clustering', y='value', hue='n')\n",
    "    ax.set_xticks([], minor=False)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    ax.grid(which='minor', axis='x')\n",
    "    plt.xlabel(None)\n",
    "    plt.legend(bbox_to_anchor=(0.93, 0.35, 0.25, 0.25), loc='center', title='n')\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.title(\"R.R. (lower is better)\")\n",
    "    melted.clustering.nunique()\n",
    "    ax = sns.barplot(data=melted[melted.metric == 'R.R.'], x='clustering', y='value')\n",
    "    plt.xticks(rotation=90)\n",
    "    ax.set_xticks(minor_ticks, minor=True)\n",
    "    plt.grid(True, which='minor')\n",
    "\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_FL(meta_df, result_df, fl_result_df,\n",
    "    filters=None,\n",
    "    formula='ochiai',\n",
    "    tie_breaker='max',\n",
    "    stopping_criterion=stop_at_min_dist_knee,\n",
    "    top_N=10):\n",
    "\n",
    "    has_faulty_components = meta['data_id'][meta['faulty_components'].apply(lambda l: len(l) > 0)]\n",
    "\n",
    "    fl_result_df = fl_result_df[(fl_result_df.formula == formula) & (fl_result_df.tie_breaker == tie_breaker)]\n",
    "\n",
    "    if filters is None:\n",
    "        fl_result_df['num_clusters'] = fl_result_df['labels_pred'].apply(lambda t: len(set(t)))\n",
    "        prediction_ranks = fl_result_df[fl_result_df.num_clusters == 1].set_index(['data_id', 'faulty_component_index'])[['cluster', 'rank']].astype('int32')\n",
    "        method = 'No Clustering'\n",
    "        assert prediction_ranks['rank'].isnull().sum() == 0\n",
    "    else:\n",
    "        tdf = filtering(result_df, [filters])\n",
    "        N = tdf.data_id.unique().shape[0]\n",
    "        if filters['clustering'] == 'Agglomerative':\n",
    "            predictions = stopping_criterion(tdf)\n",
    "        else:\n",
    "            predictions = tdf\n",
    "        assert predictions.shape[0] == N\n",
    "        \n",
    "        prediction_ranks = predictions.join(fl_result_df.set_index(['data_id', 'labels_pred']), on=['data_id', 'labels_pred'])\n",
    "        method = prediction_ranks['method'].values[0]\n",
    "        prediction_ranks = prediction_ranks[prediction_ranks.data_id.isin(has_faulty_components)]\n",
    "        #print(prediction_ranks[prediction_ranks['rank'].isnull()])\n",
    "        assert prediction_ranks['rank'].isnull().sum() == 0\n",
    "        num_ranks = prediction_ranks.groupby(['data_id', 'faulty_component_index']).count()['cluster'].reset_index()\n",
    "        for k, g in num_ranks.groupby(['data_id']):\n",
    "            arr = g.cluster.values\n",
    "            # Check if all items in an array are equal\n",
    "            assert np.max(arr) == np.min(arr)\n",
    "\n",
    "        prediction_ranks = prediction_ranks.sort_values(['data_id', 'faulty_component_index'])\\\n",
    "            .set_index(['data_id', 'faulty_component_index'])[['cluster', 'rank']]#.astype('int32')\n",
    "\n",
    "    root_cause_df = pd.DataFrame(columns=['data_id', 'fault_index', 'faulty_component_index'])\n",
    "    for index, row in meta_df.iterrows():\n",
    "        for fault_index, faulty_components_list in enumerate(row['root_cause_map']):\n",
    "            for faulty_component_index in faulty_components_list:\n",
    "                root_cause_df = root_cause_df.append({\n",
    "                    'data_id': row['data_id'],\n",
    "                    'fault_index': fault_index,\n",
    "                    'faulty_component_index': faulty_component_index\n",
    "                }, ignore_index=True)\n",
    "\n",
    "    prediction_ranks = root_cause_df.join(prediction_ranks, on=['data_id', 'faulty_component_index'])\n",
    "    num_total_faults = prediction_ranks.groupby(['data_id']).fault_index.nunique()\n",
    "    num_total_faults = num_total_faults.rename(\"num_total_faults\")\n",
    "    prediction_ranks = prediction_ranks[~prediction_ranks['rank'].isnull()]\n",
    "\n",
    "\n",
    "    first_fixed = prediction_ranks.groupby(['data_id', 'cluster'])['rank'].min().to_frame()\\\n",
    "        .reset_index().set_index(['data_id', 'cluster', 'rank']).join(\n",
    "            prediction_ranks.set_index(['data_id', 'cluster', 'rank']),\n",
    "            how='left'\n",
    "        ).reset_index(level=['rank'])\n",
    "\n",
    "    num_rankings_per_data = prediction_ranks.groupby('data_id').cluster.nunique()\n",
    "\n",
    "    topmost = prediction_ranks.sort_values('rank').groupby(['data_id', 'faulty_component_index']).first()\n",
    "\n",
    "    row = {\n",
    "        'clustering': method if 'Agglomerative' not in method else method.replace('Agglomerative', 'AHC'),\n",
    "        'R.R.': 1 - (topmost.groupby('data_id').cluster.nunique()/num_rankings_per_data).mean()\n",
    "    }\n",
    "\n",
    "    for n in top_N + ['inf']:\n",
    "        if n != 'inf':\n",
    "            num_first_fixed = first_fixed[first_fixed['rank'] <= n].groupby(['data_id']).fault_index.nunique().rename(\"num_fixed_faults\")\n",
    "        else:\n",
    "            num_first_fixed = first_fixed.groupby(['data_id']).fault_index.nunique().rename(\"num_fixed_faults\")\n",
    "        joined = pd.concat([num_total_faults, num_first_fixed], axis=1, join='outer')\n",
    "        joined.loc[joined.num_fixed_faults.isnull(), 'num_fixed_faults'] = 0\n",
    "        if n != 'inf':\n",
    "            first_fixed[f'wef_{n}'] = first_fixed['rank'].apply(lambda r: r - 1 if r <= n else n)\n",
    "        else:\n",
    "            first_fixed[f'wef_{n}'] = first_fixed['rank'] - 1\n",
    "        row[f\"found_faults (n={n})\"] = (joined['num_fixed_faults']/joined['num_total_faults']).mean()\n",
    "        row[f\"t_wef (n={n})\"] = first_fixed.groupby(['data_id', 'cluster'])\\\n",
    "                                .min()[f'wef_{n}'].groupby(['data_id']).sum().mean()\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "baselines = [\n",
    "    None, {'clustering': 'MSeer'}\n",
    "]\n",
    "if Lang == 'Java':\n",
    "    N=[1, 5, 10, 15]\n",
    "    baselines.append({'clustering': 'TCN'})\n",
    "else:\n",
    "    N=[1, 5, 10, 15]\n",
    "\n",
    "for formula in ['ochiai', 'crosstab']:\n",
    "    for tie_breaker in ['min', 'max']:\n",
    "        print(formula, tie_breaker)\n",
    "        fl_eval_df = pd.DataFrame(\n",
    "            columns=['clustering', 'R.R.'] + sum([[f\"found_faults (n={n})\", f\"t_wef (n={n})\"] for n in N+['inf']], []))\n",
    "\n",
    "        print(\"Baselines\")\n",
    "        for baseline in tqdm(baselines):\n",
    "            row = evaluate_FL(meta, result, fl_result, formula=formula, tie_breaker=tie_breaker, top_N=N, filters=baseline)\n",
    "            fl_eval_df = fl_eval_df.append(row, ignore_index=True)\n",
    "\n",
    "        affinities = ['jaccard', 'dice', 'cosine', 'hamming', 'RKT', 'hdist']\n",
    "        linkages = ['single', 'average', 'complete']\n",
    "        for affinity, linkage in tqdm(list(product(affinities, linkages))):\n",
    "            row = evaluate_FL(\n",
    "                meta, result, fl_result, formula=formula, tie_breaker=tie_breaker, top_N=N,\n",
    "                filters={'clustering': 'Agglomerative', 'affinity': affinity, 'linkage': linkage},\n",
    "                stopping_criterion=stop_at_min_dist_knee\n",
    "            )\n",
    "            fl_eval_df = fl_eval_df.append(row, ignore_index=True)\n",
    "        print(fl_eval_df.round(3).to_markdown(index=False))\n",
    "        #print(fl_eval_df.round(2).to_latex(index=False))\n",
    "        draw_figure(fl_eval_df, savepath=f\"figures/eval/FL_{formula}_{tie_breaker}_{Lang}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('.venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "5d1580c2ca67e1f666c1f1996a9358039af46ec5c42885dc0a7dc2d1ec5d5cad"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}